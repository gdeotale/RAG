{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1aa3a25-3c6c-47e8-a5f5-845014e4eab7",
      "metadata": {
        "id": "d1aa3a25-3c6c-47e8-a5f5-845014e4eab7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk....\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f36405-4ed1-4b36-9f8d-00c094de7ba0",
      "metadata": {
        "id": "76f36405-4ed1-4b36-9f8d-00c094de7ba0",
        "outputId": "daafee54-37f6-485e-e1bf-3fc8e06c6af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf in /opt/conda/lib/python3.10/site-packages (1.26.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c22acc-a62f-4670-a0be-9eba2a414bc0",
      "metadata": {
        "id": "36c22acc-a62f-4670-a0be-9eba2a414bc0"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.document_loaders import PyMuPDFLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4603791f-4d4f-4a99-9c9b-8f466b45e40b",
      "metadata": {
        "id": "4603791f-4d4f-4a99-9c9b-8f466b45e40b"
      },
      "source": [
        "# load pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c74b39-a337-444d-a7d9-d543e065c8d1",
      "metadata": {
        "id": "31c74b39-a337-444d-a7d9-d543e065c8d1"
      },
      "outputs": [],
      "source": [
        "loader = PyMuPDFLoader(\"fewshot_learning.pdf\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deef59de-8cdc-401d-a78f-ef5544392521",
      "metadata": {
        "id": "deef59de-8cdc-401d-a78f-ef5544392521",
        "outputId": "d2fd989f-125f-4576-d0cd-6de9634a134a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'producer': 'pdfTeX-1.40.17',\n",
              " 'creator': 'LaTeX with hyperref package',\n",
              " 'creationdate': '2020-07-24T00:04:08+00:00',\n",
              " 'source': 'fewshot_learning.pdf',\n",
              " 'file_path': 'fewshot_learning.pdf',\n",
              " 'total_pages': 75,\n",
              " 'format': 'PDF 1.5',\n",
              " 'title': '',\n",
              " 'author': '',\n",
              " 'subject': '',\n",
              " 'keywords': '',\n",
              " 'moddate': '2020-07-24T00:04:08+00:00',\n",
              " 'trapped': '',\n",
              " 'modDate': 'D:20200724000408Z',\n",
              " 'creationDate': 'D:20200724000408Z',\n",
              " 'page': 0}"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "469094c8-8009-423d-9d59-e696dcd35731",
      "metadata": {
        "id": "469094c8-8009-423d-9d59-e696dcd35731"
      },
      "source": [
        "# text splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41eb4c7b-e57b-4010-87c4-7d23bdcd2df1",
      "metadata": {
        "id": "41eb4c7b-e57b-4010-87c4-7d23bdcd2df1"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d4e423-c226-4638-ba63-05a702bba84a",
      "metadata": {
        "id": "82d4e423-c226-4638-ba63-05a702bba84a",
        "outputId": "a0abbfcb-808c-434b-c350-4512ee56e06a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Language Models are Few-Shot Learners\\nTom B. Brown∗\\nBenjamin Mann∗\\nNick Ryder∗\\nMelanie Subbiah∗\\nJared Kaplan†\\nPrafulla Dhariwal\\nArvind Neelakantan\\nPranav Shyam\\nGirish Sastry\\nAmanda Askell\\nSandhini Agarwal\\nAriel Herbert-Voss\\nGretchen Krueger\\nTom Henighan\\nRewon Child\\nAditya Ramesh\\nDaniel M. Ziegler\\nJeffrey Wu\\nClemens Winter\\nChristopher Hesse\\nMark Chen\\nEric Sigler\\nMateusz Litwin\\nScott Gray\\nBenjamin Chess\\nJack Clark\\nChristopher Berner\\nSam McCandlish\\nAlec Radford\\nIlya Sutskever\\nDario Amodei\\nOpenAI'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "083c002c-8bf1-44bc-a580-c5a0afbcabf9",
      "metadata": {
        "id": "083c002c-8bf1-44bc-a580-c5a0afbcabf9"
      },
      "source": [
        "# embeddings and vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80673ca5-c70e-4c66-89bd-e2abb1186d34",
      "metadata": {
        "id": "80673ca5-c70e-4c66-89bd-e2abb1186d34"
      },
      "outputs": [],
      "source": [
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vector_store = FAISS.from_documents(chunks, embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad0b515-5ec2-4c9d-be12-7c003d7180d4",
      "metadata": {
        "id": "9ad0b515-5ec2-4c9d-be12-7c003d7180d4"
      },
      "source": [
        "# vector store as retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13150b7d-eedb-4c34-9fb0-88d4115c13e4",
      "metadata": {
        "id": "13150b7d-eedb-4c34-9fb0-88d4115c13e4",
        "outputId": "df3cbd63-86a9-44f4-feb3-0f56bedc9261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(id='d0b16b94-339b-4f31-b7e6-bd596910fb63', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'fewshot_learning.pdf', 'file_path': 'fewshot_learning.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 41}, page_content='broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work.\\nDario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal,\\nAmanda Askell, Girish Sastry, and Jack Clark wrote the paper.\\nSam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work.\\nAlec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated'), Document(id='2aa40576-4fb3-4278-8d06-8f1882e98c89', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'fewshot_learning.pdf', 'file_path': 'fewshot_learning.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 41}, page_content='Ariel Herbert-Voss conducted the threat analysis of malicious use.\\nGretchen Krueger edited and red-teamed the policy sections of the paper.\\nBenjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner\\noptimized OpenAI’s clusters to run the largest models efﬁciently.\\nScott Gray developed fast GPU kernels used during training.\\nJack Clark led the analysis of ethical impacts — fairness and representation, human assessments of the model, and'), Document(id='7b6e54a6-f419-4674-8c14-15c14c23c691', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'fewshot_learning.pdf', 'file_path': 'fewshot_learning.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 74}, page_content='[ZSW+19b] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Chris-\\ntiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593,\\n2019.\\n75'), Document(id='bf4a8b3d-5bc2-4d75-bb7f-7ca05e636a47', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'fewshot_learning.pdf', 'file_path': 'fewshot_learning.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 73}, page_content='preprint arXiv:1810.12885, 2018.\\n[ZSW+19a] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul\\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2019.\\n74'), Document(id='6372387b-90bd-4054-a0af-7e60ca6c87e3', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'fewshot_learning.pdf', 'file_path': 'fewshot_learning.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 40}, page_content='language systems.\\nAcknowledgements\\nThe authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub\\nPachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea\\nVoss for helping run evaluations on OpenAI’s infrastructure. Thanks to David Luan for initial support in scaling up\\nthis project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura')]\n"
          ]
        }
      ],
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",     # or \"similarity\" or mmr\n",
        "    search_kwargs={\"k\": 5}\n",
        ")\n",
        "\n",
        "docs = retriever.invoke(\"Who wrote the paper?\")\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b504aa-86d4-49b2-bacb-8162b3878857",
      "metadata": {
        "id": "f0b504aa-86d4-49b2-bacb-8162b3878857",
        "outputId": "3102449f-c451-454b-f787-49c649ccbec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(id='4df70a45-3f9e-4610-8fec-1132222b24de', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'fewshot_learning.pdf', 'file_path': 'fewshot_learning.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 6}, page_content='zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different\\nproblem settings which offer a varying trade-off between performance on speciﬁc benchmarks and sample efﬁciency.\\nWe especially highlight the few-shot results as many of them are only slightly behind state-of-the-art ﬁne-tuned models.\\nUltimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance,'), Document(id='2a43aa4a-c5e6-43c6-bfbc-646717440913', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'fewshot_learning.pdf', 'file_path': 'fewshot_learning.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 6}, page_content='Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional ﬁne-tuning. The panels above show\\nfour methods for performing a task with a language model – ﬁne-tuning is the traditional method, whereas zero-, one-,\\nand few-shot, which we study in this work, require the model to perform the task with only forward passes at test\\ntime. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task'), Document(id='715f2d67-2e38-4793-b230-c81d5122448e', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'fewshot_learning.pdf', 'file_path': 'fewshot_learning.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 5}, page_content='ML [HYC01, VBL+16] – both involve learning based on a broad distribution of tasks (in this case implicit in\\nthe pre-training data) and then rapidly adapting to a new task.\\n• One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural\\nlanguage description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and\\nzero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.'), Document(id='8dae061d-dc24-47e2-b746-d071fe57071b', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'fewshot_learning.pdf', 'file_path': 'fewshot_learning.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 6}, page_content='descriptions, examples and prompts can be found in Appendix G.\\n• Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given\\na natural language instruction describing the task. This method provides maximum convenience, potential for\\nrobustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of')]\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "docs = retriever.invoke(\"Compare zeroshot, one shott and few shot learning\")\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36d88990-c1c5-4c9a-a4ce-4bce61c49ee1",
      "metadata": {
        "id": "36d88990-c1c5-4c9a-a4ce-4bce61c49ee1"
      },
      "source": [
        "# Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0f61936-5fbb-43bf-87ea-36c295e937b0",
      "metadata": {
        "id": "a0f61936-5fbb-43bf-87ea-36c295e937b0"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d958d608-8420-408e-a609-2b91b82ec9ef",
      "metadata": {
        "id": "d958d608-8420-408e-a609-2b91b82ec9ef"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5345b67d-0484-4a0a-8088-d96979978763",
      "metadata": {
        "id": "5345b67d-0484-4a0a-8088-d96979978763"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "      You are a helpful assistant.\n",
        "      Answer ONLY from the provided transcript context.\n",
        "      If the context is insufficient, just say you don't know.\n",
        "\n",
        "      {context}\n",
        "      Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables = ['context', 'question']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd48ab92-ea10-48db-b16d-40716324f052",
      "metadata": {
        "id": "fd48ab92-ea10-48db-b16d-40716324f052"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Step to get docs from retriever and join as a string\n",
        "def get_context(question: str):\n",
        "    docs = retriever.invoke(question)\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "parallel_chain = RunnableMap({\n",
        "    \"question\": RunnablePassthrough(),       # keep original question\n",
        "    \"context\": get_context         # fetch & format docs\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a5c02f-9758-4d30-97d4-ab907a324c1d",
      "metadata": {
        "id": "87a5c02f-9758-4d30-97d4-ab907a324c1d"
      },
      "outputs": [],
      "source": [
        "main_chain = parallel_chain | prompt | llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38572be6-affe-4a6a-8008-f242b476c09b",
      "metadata": {
        "id": "38572be6-affe-4a6a-8008-f242b476c09b",
        "outputId": "ecbd1ae6-3fa5-4ff2-9ba3-a93e61f68fd0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The paper discusses the training and evaluation of GPT-3, organized into several sections. \\n\\n1. **Introduction**: It sets the stage for the importance of informal conversation in business relationships and the need to understand cultural sensitivities.\\n\\n2. **Approach**: This section details the methods used for training GPT-3 and evaluating its performance.\\n\\n3. **Results**: It presents findings on various tasks in zero-, one-, and few-shot settings.\\n\\n4. **Data Contamination**: This section addresses concerns regarding train-test overlap.\\n\\n5. **Limitations**: It discusses the weaknesses of GPT-3 in text synthesis and several NLP tasks, despite improvements over its predecessor, GPT-2.\\n\\n6. **Broader Impacts**: This section explores the wider implications of the technology.\\n\\n7. **Related Work**: It reviews existing literature relevant to the study.\\n\\n8. **Conclusion**: The paper wraps up the findings and discussions.\\n\\nAdditionally, the authors include specific figures illustrating task formatting and phrasing, and they emphasize that all data comes from ground truth datasets without samples from GPT-3. Key contributors to the paper include Dario Amodei, Alec Radford, and others, with Sam McCandlish leading the analysis of model scaling.'"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "main_chain.invoke('Describe the paper in detail?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa415a93-1545-43dc-8706-822cc87f5d2a",
      "metadata": {
        "id": "aa415a93-1545-43dc-8706-822cc87f5d2a",
        "outputId": "c4eb1771-0f03-472d-8c60-76b2b767dcd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The broader impacts of language models like GPT-3 include beneficial applications such as code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. However, there are significant concerns regarding the potential for deliberate misuse of these models, as well as issues of bias, fairness, and representation within the models. Additionally, energy efficiency is a concern due to the large amounts of computation required for training these models.'"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "main_chain.invoke('Describe the broader impacts in brief?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b143d28d-f82b-41dd-9ebc-cbd019a9464f",
      "metadata": {
        "id": "b143d28d-f82b-41dd-9ebc-cbd019a9464f",
        "outputId": "3910e176-e32b-46ed-cbde-b3e5dfb36dcc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Zero-shot, one-shot, and few-shot learning are different problem settings that offer varying trade-offs between performance on specific benchmarks and sample efficiency.\n",
              "\n",
              "- **Zero-shot learning** involves the model performing a task without any prior examples or demonstrations. The model relies solely on its pre-trained knowledge and a natural language description of the task.\n",
              "\n",
              "- **One-shot learning** allows for only one demonstration of the task, in addition to the natural language description. This setting is significant because it closely resembles how some tasks are communicated to humans.\n",
              "\n",
              "- **Few-shot learning** permits the model to see a few dozen examples before performing the task. This approach often yields results that are only slightly behind state-of-the-art fine-tuned models.\n",
              "\n",
              "The comparison highlights that while few-shot learning can achieve high performance, zero-shot and one-shot learning are often seen as fairer comparisons to human performance, as they require less reliance on examples. Each method captures different aspects of learning and adaptation, with few-shot being more sample-efficient than traditional fine-tuning methods."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "response = main_chain.invoke('compare zero shot, one shot, few shot learning in detail')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dccc036-bd35-49b2-af10-434b7c440de5",
      "metadata": {
        "id": "3dccc036-bd35-49b2-af10-434b7c440de5",
        "outputId": "fe05f432-c163-4176-eb40-f8b67c86df15"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Zero-shot, one-shot, and few-shot learning are different problem settings that offer varying trade-offs between performance on specific benchmarks and sample efficiency.\n",
              "\n",
              "- **Zero-shot learning** involves the model performing a task without any prior examples or demonstrations. The model relies solely on its pre-trained knowledge and a natural language description of the task.\n",
              "\n",
              "- **One-shot learning** allows the model to see only one demonstration of the task, in addition to the natural language description. This setting is significant because it closely resembles how some tasks are communicated to humans, making it a relevant comparison for human performance.\n",
              "\n",
              "- **Few-shot learning** permits the model to be presented with a few dozen examples of the task, along with the natural language description. This approach often yields results that are only slightly behind state-of-the-art fine-tuned models, highlighting its effectiveness.\n",
              "\n",
              "Overall, while fine-tuning is the traditional method for training models, zero-shot, one-shot, and few-shot learning require only forward passes at test time, making them more sample efficient. Each method has its own strengths and is suited for different scenarios, with few-shot learning often providing a good balance between performance and efficiency."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#with multiquery retriever\n",
        "from IPython.display import Markdown, display\n",
        "response = main_chain.invoke('compare zero shot, one shot, few shot learning in detail')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00631077-d2c8-4981-a78a-0cd788b88296",
      "metadata": {
        "id": "00631077-d2c8-4981-a78a-0cd788b88296"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}